{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 2: Neural network in AWS SageMaker\n",
    "\n",
    "- Today we are going to use AWS SageMaker to build, train and deploy a VNN. \n",
    "- Our data and goal are the same as Week 1 - we will use the Fashion MNIST data and our goal will be to contrust a simple WVV to classify the type of clothing. \n",
    "\n",
    "Our Tasks are:\n",
    "1. Understand the structure of AWS-SageMaker.\n",
    "2. How to create a instance on AWS-SageMaker.\n",
    "3. Requesting intances with larger capacity - contact AWS support team \n",
    "3. Demonstrate how to build, train, deploy and evaluate a VNN on AWS. \n",
    "4. Remeber to **stop instance and DELETE endpoint** when they finish tasks otherwise AWS will keep charging. It can be very expensive.\n",
    "\n",
    "\n",
    "### Build, train and deploy a NN model in AWS SageMaker\n",
    "Data\n",
    "- MNIST fashion data contains 60,000 small square 28 Ã— 28 pixel grayscale images of 10 types of clothing, such as shoes, t-shirts, dresses, and more.\n",
    "- All the code here is run on kernel **conda_tensorflow_p36** configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Sign up for an AWS account \n",
    "If you don't already have an account [sign up here](https://portal.aws.amazon.com/billing/signup#/start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create an AWS SageMaker instance\n",
    "\n",
    "We will create a notebook instance that is used to download and process the data. \n",
    "\n",
    "1. Sign in to the [AWS SageMaker console](https://aws.amazon.com/console/) as a Root user\n",
    "![title](pics/sagemaker_console.png)\n",
    "\n",
    "2. Navigate to Notebook instances in the left menu pane, and select Create notebook instance. \n",
    "![title](pics/create_instance.png)\n",
    "\n",
    "3. Specify your Notebook instance settings\n",
    "   - Give your new instance a suitable name *e.g.* MA5852-Lab2\n",
    "   - Select the **instance type** as **ml.t2.medium**. Note: This one is free. If you need intances with different capacity you can select it here. \n",
    "   - Leave **elastic inference** as **default selection (none)**\n",
    "   \n",
    "4. In the Permissions and encryption section, **create a new IAM role**. Leave the selections as default and select **create role**. Leave the **root access enabled as default**. \n",
    "![title](pics/notebook2.png)\n",
    "\n",
    "5. Choose **Create Notebook instance**.\n",
    "\n",
    "6. The **Notebook instances** section will now open, and the new notebook instance us displayed. The status will be *pending*, and this status will change to *InService* when the notebook is ready. \n",
    "![title](pics/new_instance.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Preparing the data\n",
    "We will now use our new notebook instance to load, prepare the Fashion MNIST data and upload the data to Amazon S3. \n",
    "\n",
    "1. When your notebook instance status changes to **InService**, select **Open Jupyter**\n",
    "![title](pics/inservice.png)\n",
    "\n",
    "Note, you can select Open JupyterLab to get a heap of tutorials etc... \n",
    "\n",
    "2. In the Notebook Instance, and either; \n",
    "    - create a new notebook using **new** and select the **kernel conda_tensorflow_p36** configuration. A new code cell will appear in your Jupyter notebook. Run the following code by copying and pasting it into your Notebook, or;\n",
    "    - upload existing jupyter notebooks & python scripts using **upload**. Select the **conda_tensorflow_p36** kernal when prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start: Check directory structure and modify permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have a lost+found folder with root group and owner? If so, you will need to change the permissions of to lost+found to prevent future errors. Note, I am in contact with AWS Support for a better solution and will keep you all updated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "sudo chown ec2-user lost+found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "sudo chgrp ec2-user lost+found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need to set up our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to import the necessary libraries and define some environment variables  \n",
    "## Import sagemaker and retrieve IAM role, which determines your user identity and permissions\n",
    "\n",
    "import sagemaker #import sagemaker\n",
    "print(sagemaker.__version__) #print the sagemaker version\n",
    "sess = sagemaker.Session() ### Manages interactions with the Amazon SageMaker APIs and \n",
    "                           ### any other AWS services needed e.g. S3\n",
    "role = sagemaker.get_execution_role() ### Get and save the IAM role as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import os, keras, numpy, pyplot and the fashion MNIST data \n",
    "\n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "from matplotlib import pyplot\n",
    "(x_train, y_train), (x_val, y_val) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a quick look at data \n",
    "\n",
    "#Each image is represented as a 28x28 pixel grayscale images\n",
    "## View shape and type of data\n",
    "xtr = x_train.shape, x_train.dtype\n",
    "ytr = y_train.shape, y_train.dtype\n",
    "\n",
    "print(\"x_train_shape & data type:\", xtr)\n",
    "print(\"y_train_shape & data type:\", ytr)\n",
    "\n",
    "# plot some raw pixel data\n",
    "for i in range(9):\n",
    "  \n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "\n",
    "    pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directory for the data and save the test and training data here\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "np.savez('./data/training', image = x_train, label=y_train)\n",
    "np.savez('./data/test', image=x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "ls -l data ## Check that the directories have been created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to train our model on the local instance - this is an optional step and is to check if our code will run on AWS. We train the model using TensorFlow() to create a tf_estimator object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use the python script that made in Lab 1 to train our VNN model. \n",
    "## If you haven't already uploaded this into your notebook instance then do that now. \n",
    "\n",
    "## We first need to get the python script from Lab 1 for the analysis. Upload into our notebook instance. \n",
    "\n",
    "#Import tensorflow from sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "## documentation https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html\n",
    "\n",
    "#Set environment variables - file paths to data and for output\n",
    "local_training_input_path = 'file://data/training.npz'\n",
    "local_test_input_path = 'file://data/test.npz'\n",
    "output = 'file:///tmp'\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='mnist_fashion_vnn_tf2.py', #path to local python source file to be executed\n",
    "                          role = role, #the IAM ROLE ARN for the model - unique user ID\n",
    "                          source_dir ='.', #path to the directory where any other dependancies are apart from entry point\n",
    "                          instance_count = 1, #the number of EC2 intances to use\n",
    "                          instance_type ='local', # Type of EC2 instance to use local = this one! \n",
    "                          framework_version = '2.1.0', # Tensorflow version for executing your tf code\n",
    "                          py_version ='py3', #version of python for executing your model training code\n",
    "                          script_mode =True, #enables us to use our python script to train the model\n",
    "                          hyperparameters={'epochs':1}, #hyperparameters used by our custom TensorFlow code during model training\n",
    "                          output_path = output) #location for saving the results. Default = saved in the default S3 bucket.\n",
    "\n",
    "#Note, Estimator is a high level interface for SageMaker training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit is used to train the model saved in the estimator object. We pass in file paths to the \n",
    "#trainng and test data (in this example they are stored locally)\n",
    "\n",
    "tf_estimator.fit({'training': local_training_input_path, 'validation': local_test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model in AWS\n",
    "\n",
    "Now we know that our code is working on SageMaker (note, we can only do this because we have a small dataset and a shallow neural network - this wouldn't work with large datasets or deep neural networks), we can train our model on a larger instance. \n",
    "\n",
    "1. Upload the dataset to S3. S3 is a default bucket for storing data and model output in AWS\n",
    "2. Select the [EC2 instance type](https://aws.amazon.com/ec2/instance-types/?trkCampaign=acq_paid_search_brand&sc_channel=PS&sc_campaign=acquisition_ANZ&sc_publisher=Google&sc_category=Cloud%20Computing&sc_country=ANZ&sc_geo=APAC&sc_outcome=acq&sc_detail=aws%20ec2%20instance%20pricing&sc_content={ad%20group}&sc_matchtype=e&sc_segment=489278081276&sc_medium=ACQ-P|PS-GO|Brand|Desktop|SU|Cloud%20Computing|EC2|ANZ|EN|Sitelink&s_kwcid=AL!4422!3!489278081276!e!!g!!aws%20ec2%20instance%20pricing&ef_id=Cj0KCQjw1PSDBhDbARIsAPeTqrdxZQ3nAtQNtB_MzOowvGLxppgm3YnqP08nDUrv8ubtE_Y19XwRNIcaAkaXEALw_wcB:G:s&s_kwcid=AL!4422!3!489278081276!e!!g!!aws%20ec2%20instance%20pricing) for your model. For this subject, we mainly use *ml.m4.xlarge*. EC stands for Elastic Compute Clous, and its a web service where AWS subscribers can request and provision compute services in the AWS cloud. You'll be charged per hour with different rates, depending on the instance you choose. Don't forget to terminate the instance when you're done to stop being over-charged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload data to S3 bucket\n",
    "## Note - we get a certain capacity for free, after that you are charged\n",
    "\n",
    "prefix = 'keras-mnist-fashion' #first define a prefix for the key (think of this like a directory or file path)\n",
    "\n",
    "#upload a local file/directory to S3 using upload_data(). \n",
    "##inputs = path, bucket (if not specifified will use default_bucket), optional prefix for directory structure\n",
    "training_input_path = sess.upload_data('data/training.npz', key_prefix = prefix+'/training')\n",
    "\n",
    "test_input_path = sess.upload_data('data/test.npz', key_prefix = prefix+'/validation')\n",
    "\n",
    "print(training_input_path)\n",
    "print(test_input_path) ### note - you can look at your buckets in the S3 section of AWS. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with managed instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used [managed spot instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html) to save money. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point='mnist_fashion_vnn_tf2.py',  #Python script\n",
    "                          source_dir = '.',\n",
    "                          role=role,\n",
    "                          instance_count=1, \n",
    "                          instance_type='ml.m4.xlarge', # instance type\n",
    "                          framework_version='2.1.0', # Tensorflow version\n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={'epochs': 3},\n",
    "                          ## after this line, everything is optional for managed spot instance\n",
    "                          use_spot_instances=True,        # Use spot instance\n",
    "                          max_run=3600,                    # Max training time\n",
    "                          max_wait=7200,                  # Max training time + spot waiting time\n",
    "                         ) ##note for martha ## means optional - money saving. only downside, if under specificy then will kill job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': training_input_path, 'validation': test_input_path})   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model\n",
    "\n",
    "- Model deployment means to expose the model to real use.\n",
    "- This means you can make inferences or predictions using your model, \n",
    "- The model is deployed in an EC2 instance \n",
    "- Deployment is via Amazon SageMaker endpoints â€“ an Amazon SageMaker endpoint is a fully managed service that allows you to make real-time inferences via a REST API. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tf_endpoint_name = 'keras-tf-fmnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()) #give the endpoint a name.\n",
    "#used the time and date from the time library\n",
    "\n",
    "#deploy() Deploys the Model to an Endpoint and optionally return a Predictor.\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1, # The initial number of instances to run in the Endpoint created from this Model.\n",
    "                                   instance_type='ml.m4.xlarge', # The EC2 instance type to deploy this Model to.\n",
    "                                   endpoint_name=tf_endpoint_name) # The name of the endpoint to create     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  %matplotlib inline\n",
    "import random #random number generator for random sampling\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "\n",
    "#select 10 of the test samples (images) randomly\n",
    "num_samples = 10\n",
    "indices = random.sample(range(x_val.shape[0] - 1), num_samples)\n",
    "images = x_val[indices]/255\n",
    "labels = y_val[indices]\n",
    "\n",
    "for i in range(num_samples): #plot them with their labels \n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Generate predictions for those random test images\n",
    "# Apply the preductor() function to a Predictor oject\n",
    "# It returns inferences for the given input - in this case the images\n",
    "\n",
    "prediction = tf_predictor.predict(images.reshape(num_samples, 28, 28, 1))['predictions']\n",
    "prediction = np.array(prediction) #save the predictions as a np.array (softmax decimal probabilties)\n",
    "print(prediction)\n",
    "predicted_labels = prediction.argmax(axis=1) #use argmax to turn the predictions into class labels\n",
    "print('Predicted labels are: {}'.format(predicted_labels)) # print out the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "\n",
    "Remember to delete the endpoint when you are not using to avoid unnecessary surcharge from AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
