{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Host a Keras Model on Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker is a fully-managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK makes it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks, including TensorFlow and Keras.\n",
    "\n",
    "In this notebook, we train and host a [Keras Sequential model](https://keras.io/getting-started/sequential-model-guide) on SageMaker. The model used for this notebook is a simple multi-layer perceptron neural network (VNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First check the directory structure and modify permissions if a lost+found folder is present with root group and/or owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l\n",
    "\n",
    "%%sh\n",
    "sudo chown ec2-user lost+found\n",
    "\n",
    "%%sh\n",
    "ls -l\n",
    "\n",
    "%%sh\n",
    "sudo chgrp ec2-user lost+found\n",
    "\n",
    "%%sh\n",
    "ls -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next define a few variables that will be needed later. Don't forget to change the kernel to **conda_tensorflow_p36**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "The [MNIST dataset](https://deepai.org/dataset/mnist) is a low-complexity data collection of hand-written digits used to train and test various supervised machine learning algorithmsm. It is also considered to be the \"Hello, World!\" of machine learning. The database contains 70,000 28x28 black and white images representing the digits zero through nine. It is split into two subsets, with 60,000 images belonging to the training set and 10,000 images belonging to the testing set. The separation of images ensures that given what an adequately trained model has learned previously, it can accurately classify relevant images not previously examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os, keras, numpy, pyplot and the MNIST data \n",
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# mnist = tf.keras.datasets.mnist # get mnist from keras\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a quick look at data \n",
    "\n",
    "#Each image is represented as a 28x28 pixel grayscale images\n",
    "## View shape and type of data\n",
    "xtr = x_train.shape, x_train.dtype\n",
    "ytr = y_train.shape, y_train.dtype\n",
    "\n",
    "print(\"x_train_shape & data type:\", xtr)\n",
    "print(\"y_train_shape & data type:\", ytr)\n",
    "\n",
    "# plot some raw pixel data\n",
    "for i in range(9):  \n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(x_train[i], cmap=pyplot.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local directory for the data and save the training and test data there\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "np.savez('./data/training', image = x_train, label=y_train)\n",
    "np.savez('./data/test', image=x_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "ls -l data ## Check that the directories have been created and the files have been saved successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next train the model on the local instance - this is an optional step and is to check if the code will run on AWS. The model is trained using TensorFlow() to create a tf_estimator object.\n",
    "\n",
    "In more detail, before running the baseline training job, [the SageMaker Python SDK's Local Mode feature](https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode) is first used to check that the code works with SageMaker's TensorFlow environment. Local Mode downloads the [prebuilt Docker image for TensorFlow](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) and runs a Docker container locally for a training job. A TensorFlow estimator is created, and the `instance_type` is specified as to be `'local'` or `'local_gpu'`, depending on the local instance type. This tells the estimator to run the training job locally (as opposed to on SageMaker). The  training code is also only run for only one epoch because the intent is to verify the code, not train an accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to upload the python script into the same notebook instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow from sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# Set environment variables - file paths to data and for output\n",
    "local_training_input_path = 'file://data/training.npz'\n",
    "local_test_input_path = 'file://data/test.npz'\n",
    "output = 'file:///output'\n",
    "\n",
    "instance_type = \"local\"\n",
    "\n",
    "if subprocess.call(\"nvidia-smi\") == 0:\n",
    "    instance_type = \"local_gpu\" # set instance_type to GPU if one is present\n",
    "    \n",
    "local_hyperparameters = {\"epochs\": 1, \"batch-size\": 64}\n",
    "\n",
    "tf_estimator = TensorFlow(entry_point='mnist_vnn_tf2.py', # path to local python source file to be executed\n",
    "                          role = role, # the IAM ROLE ARN for the model - unique user ID\n",
    "                          source_dir ='.', # path to the directory where any other dependancies are apart from entry point\n",
    "                          instance_count = 1, #the number of EC2 intances to use\n",
    "                          instance_type = instance_type, # Type of EC2 instance to use local = this one! \n",
    "                          framework_version = '2.1.0', # Tensorflow version for executing the tf code\n",
    "                          py_version ='py3',\n",
    "                          script_mode =True,\n",
    "                          hyperparameters=local_hyperparameters,\n",
    "                          output_path = output) # location for saving the results. Default = saved in the default S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit is used to train the model saved in the estimator object. The local files paths to the traiing and test data also \n",
    "# need to be passed in\n",
    "tf_estimator.fit({'training': local_training_input_path, 'validation': local_test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model in AWS\n",
    "\n",
    "Now that it has been determined the code is working on SageMaker (note, this is only possible because it's a small dataset and a shallow neural network - it won't work with large datasets or deep neural networks), the model can be trained on a larger instance. \n",
    "\n",
    "1. Upload the dataset to S3. S3 is a default bucket for storing data and model output in AWS\n",
    "2. Select the [EC2 instance type](https://aws.amazon.com/ec2/instance-types/) for the model. MA5852 will mainly use *ml.m4.xlarge*. EC stands for Elastic Compute Cloud, and its a web service where AWS subscribers can request and provision compute services in the AWS cloud. The user is charged per hour with different rates, depending on the instance chosen. Don't forget to terminate the instance when done to stop being over-charged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "prefix = 'keras-mnist'\n",
    "\n",
    "training_input_path = sess.upload_data('data/training.npz', key_prefix = prefix+'/training')\n",
    "test_input_path = sess.upload_data('data/test.npz', key_prefix = prefix+'/validation')\n",
    "\n",
    "print(training_input_path)\n",
    "print(test_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {\"epochs\": 10, \"batch-size\": 256}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"mnist_vnn_tf2.py\",\n",
    "    role = role,\n",
    "    source_dir='.',\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    framework_version=\"2.1.0\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    script_mode = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'training': training_input_path, 'validation': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the trained model\n",
    "\n",
    "After the model is trained, it can be deployed to a SageMaker Endpoint, which serves prediction requests in real-time. To do so, simply call `deploy()` on the estimator, passing in the desired number of instances and instance type for the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tf_endpoint_name = 'keras-tf-mnist-'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()) #give the endpoint a name.\n",
    "# used the time and date from the time library\n",
    "\n",
    "# deploy() deploys the model to an endpoint and optionally return a predictor.\n",
    "tf_predictor = tf_estimator.deploy(initial_instance_count=1, # The initial number of instances to run in the endpoint created from this Model.\n",
    "                                   instance_type='ml.m4.xlarge', # The EC2 instance type to deploy this model to.\n",
    "                                   endpoint_name=tf_endpoint_name) # The name of the endpoint to create   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the test dataset for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def predict(data):\n",
    "    predictions = predictor.predict(data)[\"predictions\"]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "predicted = []\n",
    "actual = []\n",
    "batches = 0\n",
    "batch_size = 128\n",
    "\n",
    "datagen = ImageDataGenerator()\n",
    "for data in datagen.flow(x_test, y_test, batch_size=batch_size):\n",
    "    for i, prediction in enumerate(predict(data[0])):\n",
    "        predicted.append(np.argmax(prediction))\n",
    "        actual.append(data[1][i][0])\n",
    "\n",
    "    batches += 1\n",
    "    if batches >= len(x_test) / batch_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the predictions to calculate model accuracy and create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_pred=predicted, y_true=actual)\n",
    "display(\"Average accuracy: {}%\".format(round(accuracy * 100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_pred=predicted, y_true=actual)\n",
    "cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "sn.set(rc={\"figure.figsize\": (11.7, 8.27)})\n",
    "sn.set(font_scale=1.4)  # for label size\n",
    "sn.heatmap(cm, annot=True, annot_kws={\"size\": 10})  # font size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up - Delete the endpoint\n",
    "\n",
    "Remember to delete the endpoint to avoid unnecessary surcharge from AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
