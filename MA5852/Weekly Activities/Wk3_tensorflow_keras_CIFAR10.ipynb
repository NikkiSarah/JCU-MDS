{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Host a Keras Model on Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker is a fully-managed service that provides developers and data scientists with the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models. The SageMaker Python SDK makes it easy to train and deploy models in Amazon SageMaker with several different machine learning and deep learning frameworks, including TensorFlow and Keras.\n",
    "\n",
    "In this notebook, we train and host a [Keras Sequential model](https://keras.io/getting-started/sequential-model-guide) on SageMaker. The model used for this notebook is a simple multi-layer perceptron neural network (VNN) that was based on [the Keras examples](https://www.tensorflow.org/tutorials/images/cnn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We first need to check the directory structure and modify permissions if a lost+found folder is present with root group and/or owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l\n",
    "\n",
    "%%sh\n",
    "sudo chown ec2-user lost+found\n",
    "\n",
    "%%sh\n",
    "ls -l\n",
    "\n",
    "%%sh\n",
    "sudo chgrp ec2-user lost+found\n",
    "\n",
    "%%sh\n",
    "ls -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a few variables that will be needed later. Don't forget to change the kernel to **conda_tensorflow_p36**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "The [MNIST dataset](https://deepai.org/dataset/mnist) is a low-complexity data collection of hand-written digits used to train and test various supervised machine learning algorithmsm. It is also considered to be the \"Hello, World!\" of machine learning. The database contains 70,000 28x28 black and white images representing the digits zero through nine. It is split into two subsets, with 60,000 images belonging to the training set and 10,000 images belonging to the testing set. The separation of images ensures that given what an adequately trained model has learned previously, it can accurately classify relevant images not previously examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "\n",
    "1. In the Jupyter Notebook, download the MNIST data (or use the tensorflow.dataset).\n",
    "2. Modify the upload to S3 for your new data then upload to S3.\n",
    "3. In the Python file containing the Keras model:\n",
    "- Change the Keras model to the simple mutli-layer perceptron model used in Week 1\n",
    "- Modify the train_input_fn, eval_input_fn to use the MNIST data\n",
    "- Modify the _input to remove the padding, trimming and flipping effects (for now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "mnist = tf.keras.datasets.mnist # get mnist from keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local directories for the data\n",
    "os.makedirs(\"./data\", exist_ok=True) #exist_ok = True means no error if directory already exists\n",
    "os.makedirs('./data/training', exist_ok=True)\n",
    "os.makedirs('./data/test', , exist_ok=True)\n",
    "os.makedirs(\"./output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save the training and test data\n",
    "np.savez('./data/training', image = x_train, label=y_train)\n",
    "np.savez('./data/test', image=x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -l data # check they have been created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the data to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "dataset_uri = S3Uploader.upload(\"data\", \"s3://{}/Wk3Activity6-mnist/data\".format(bucket))\n",
    "\n",
    "display(dataset_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In this activity, we train a VNN to learn a classification task with the mnist dataset.\n",
    "\n",
    "### Run a baseline training job on SageMaker\n",
    "\n",
    "The SageMaker Python SDK's `sagemaker.tensorflow.TensorFlow` estimator class makes it easy for us to interact with SageMaker. We create one for each of the different training jobs we run in this example. A couple parameters worth noting:\n",
    "\n",
    "* `entry_point`: our training script (adapted from [this Keras example](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)).\n",
    "* `train_instance_count`: the number of training instances. Here, we set it to 1 for our baseline training job.\n",
    "\n",
    "As we run each of our training jobs, we change different parameters to configure our different training jobs.\n",
    "\n",
    "For more details about the TensorFlow estimator class, see the [API documentation](https://sagemaker.readthedocs.io/en/stable/sagemaker.tensorflow.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the training code\n",
    "\n",
    "Before running the baseline training job, we first use [the SageMaker Python SDK's Local Mode feature](https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode) to check that our code works with SageMaker's TensorFlow environment. Local Mode downloads the [prebuilt Docker image for TensorFlow](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) and runs a Docker container locally for a training job. In other words, it simulates the SageMaker environment for a quicker development cycle, so we use it here just to test out our code.\n",
    "\n",
    "We create a TensorFlow estimator, and specify the `instance_type` to be `'local'` or `'local_gpu'`, depending on our local instance type. This tells the estimator to run our training job locally (as opposed to on SageMaker). We also have our training code run for only one epoch because our intent here is to verify the code, not train an accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "instance_type = \"local\"\n",
    "\n",
    "if subprocess.call(\"nvidia-smi\") == 0:\n",
    "    # Set instance type to GPU if one is present\n",
    "    instance_type = \"local_gpu\"\n",
    "\n",
    "local_hyperparameters = {\"epochs\": 1, \"batch-size\": 64}\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"cifar10_keras_main.py\",\n",
    "    source_dir=\"source_dir\", # other option is \".\"\n",
    "    role=role,\n",
    "    framework_version=\"2.1.0\", # updated from 1.15.2\n",
    "    py_version=\"py3\",\n",
    "    script_mode=True,\n",
    "    hyperparameters=local_hyperparameters,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our estimator, we call `fit()` to start the training job and pass the inputs that we downloaded earlier. We pass the inputs as a dictionary to define different data channels for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "local_inputs = {\n",
    "    \"train\": \"file://{}/train\".format(data_path),\n",
    "    \"validation\": \"file://{}/test\".format(data_path),\n",
    "    \"eval\": \"file://{}/eval\".format(data_path),\n",
    "}\n",
    "estimator.fit(local_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a baseline training job on SageMaker\n",
    "\n",
    "Now we run training jobs on SageMaker, starting with our baseline training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure metrics\n",
    "\n",
    "In addition to running the training job, Amazon SageMaker can retrieve training metrics directly from the logs and send them to CloudWatch metrics. Here, we define metrics we would like to observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \".*loss: ([0-9\\\\.]+) - accuracy: [0-9\\\\.]+.*\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \".*loss: [0-9\\\\.]+ - accuracy: ([0-9\\\\.]+).*\"},\n",
    "    {\n",
    "        \"Name\": \"validation:accuracy\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: ([0-9\\\\.]+).*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"validation:loss\",\n",
    "        \"Regex\": \".*step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: ([0-9\\\\.]+) - val_accuracy: [0-9\\\\.]+.*\",\n",
    "    },\n",
    "    {\n",
    "        \"Name\": \"sec/steps\",\n",
    "        \"Regex\": \".* - \\d+s (\\d+)[mu]s/step - loss: [0-9\\\\.]+ - accuracy: [0-9\\\\.]+ - val_loss: [0-9\\\\.]+ - val_accuracy: [0-9\\\\.]+\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we create a TensorFlow estimator, with a couple key modfications from last time:\n",
    "\n",
    "* `train_instance_type`: the instance type for training. We set this to `ml.p2.xlarge` because we are training on SageMaker now. For a list of available instance types, see [the AWS documentation](https://aws.amazon.com/sagemaker/pricing/instance-types).\n",
    "* `metric_definitions`: the metrics (defined above) that we want sent to CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "hyperparameters = {\"epochs\": 10, \"batch-size\": 256}\n",
    "tags = [{\"Key\": \"Project\", \"Value\": \"cifar10\"}, {\"Key\": \"TensorBoard\", \"Value\": \"file\"}]\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"cifar10_keras_main.py\",\n",
    "    source_dir=\"source_dir\",\n",
    "    metric_definitions=metric_definitions,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    framework_version=\"1.15.2\",\n",
    "    py_version=\"py3\",\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p2.xlarge\",\n",
    "    base_job_name=\"cifar10-tf\",\n",
    "    tags=tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we call `fit()` to start the SageMaker training job and pass the inputs in a dictionary to define different data channels for training. This time, we use the S3 URI from uploading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"train\": \"{}/train\".format(dataset_uri),\n",
    "    \"validation\": \"{}/validation\".format(dataset_uri),\n",
    "    \"eval\": \"{}/eval\".format(dataset_uri),\n",
    "}\n",
    "\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the job training metrics\n",
    "\n",
    "We can now view the metrics from the training job directly in the SageMaker console.  \n",
    "\n",
    "Log into the [SageMaker console](https://console.aws.amazon.com/sagemaker/home), choose the latest training job, and scroll down to the monitor section. Alternatively, the code below uses the region and training job name to generate a URL to CloudWatch metrics.\n",
    "\n",
    "Using CloudWatch metrics, you can change the period and configure the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import parse\n",
    "\n",
    "from IPython.core.display import Markdown\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "cw_url = parse.urlunparse(\n",
    "    (\n",
    "        \"https\",\n",
    "        \"{}.console.aws.amazon.com\".format(region),\n",
    "        \"/cloudwatch/home\",\n",
    "        \"\",\n",
    "        \"region={}\".format(region),\n",
    "        \"metricsV2:namespace=/aws/sagemaker/TrainingJobs;dimensions=TrainingJobName;search={}\".format(\n",
    "            estimator.latest_training_job.name\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"CloudWatch metrics: [link]({}). After you choose a metric, \"\n",
    "        \"change the period to 1 Minute (Graphed Metrics -> Period).\".format(cw_url)\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "notice": "Copyright 2017-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
