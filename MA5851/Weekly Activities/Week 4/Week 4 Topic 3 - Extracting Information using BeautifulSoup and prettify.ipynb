{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4 Topic 3: Extracting Information using BeautifulSoup and prettify\n",
    "\n",
    "#### Web page scraping tutorial with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "200\n",
      "[<class 'bs4.element.Doctype'>, <class 'bs4.element.Tag'>]\n",
      "98\n",
      "Australian Breaking News Headlines & World News Online | SMH.com.au\n",
      "After six weeks, the Morrison government is reeling from shocking allegations of rape, sexual misconduct and more. Does it have a problem with women? And can it recover?\n",
      "A weekly newsletter for book lovers from books editor Jason Steger.\n",
      "The Booklist newsletter\n",
      "Daily Crosswords\n",
      "Greater Good newsletter\n",
      "‘I intend to own those mistakes’: Andrew Laming won’t recontest after bad behaviour put under spotlight\n",
      "Coalition hit by escalating series of missteps and scandals relating to women, sex and power\n",
      "PM Scott Morrison admits Coalition failings on sexism, bad behaviour in leaked audio\n",
      "‘Hacks, stacks and freaks’: Why do political staffers behave so badly?\n",
      "Father asked school to report teen to police. Months later, his daughter was allegedly raped\n",
      "At present rates, the gender pay gap will vanish in 268 years\n",
      "Queensland finds missing link to cluster as NSW set to ease restrictions\n",
      "Riders of the storm: Mid North Coast awash as history repeats\n",
      "‘Lack of empathy and sympathy’: Formerly stranded Aussies adjust to life back home\n",
      "LNP silences its own vice-president: ‘Do you think anyone could gag me?’ \n",
      "‘You still battle’: Rosie Batty on five years of family violence action\n",
      "On my honeymoon with Guy, a phone call tragically changed our lives\n",
      "Plant-based meat sales are soaring, but experts warn it may not be better for you or the environment\n",
      "‘Cult’ or a boon for self-love?: Inside the latest curly hair craze\n",
      "‘Freak situations’: V’landys denies new rules to blame for injury crisis\n",
      "Panthers star could face huge payout as scandal heads to Supreme Court\n",
      "Laura Macaulay’s history-making surf prepares her to face Cape Fear\n",
      "We asked readers to tell us their stories, and hundreds responded\n",
      "From One Nation to neo-Nazism: Aussies drawn to extremism\n",
      "‘I’ve really struggled’: Liberal MP recalls past abuse in call for change\n",
      "AFL 2021 LIVE updates: Dusty, Richmond off to a flyer against Hawthorn\n",
      "New faces aplenty as Rennie reveals first Wallabies squad of the year\n",
      "Eurovision disqualifies Belarus over ‘pro-crackdown’ political lyrics\n",
      "Queensland finds missing link to cluster as NSW set to ease restrictions\n",
      "Stressed out? It could be wreaking havoc in your teeth\n",
      "I gave my daughter a suitcase of vintage classics for her 18th birthday\n",
      "‘Screams howled as we flew into a flood of high beams. I’m dead, I thought’\n",
      "Residents slugged for sporting complex that ‘won’t meet minimum standards’\n",
      "Big wet prompts Forestry Corp to halt northern NSW logging operations\n",
      "Woollahra house sells for $1.5 million above reserve on Super Saturday\n",
      "Piano tutor at Sydney high school charged with historical sexual abuse\n",
      "Counting the increasing costs of droughts and flooding rains\n",
      "'Simple, yet so clever': Award-winning beach house for sale for the first time\n",
      "Family drops top dollar on house built out of 12 shipping containers\n",
      "Australia Post honours Holden with stamp collection\n",
      "Horror injury for Ben Hunt, who plays on with broken leg\n",
      "Broncos break 13-match losing streak against embarrassing Bulldogs\n",
      "‘I can’t tell you the truth’: Stuart bites his tongue after costly call\n",
      "Greens target super-rich with $40b billionaires’ wealth tax\n",
      "Vaccine scepticism in Europe filtering through to vulnerable Australians\n",
      "PBS listing slashes breast cancer treatment from $50k to hundreds of dollars\n",
      "China hits Australian winemakers with new tariffs\n",
      "The new burning question. Or is it?\n",
      "How to tell if your insurer covers you for flood damage\n",
      "A guide to budgeting and saving for a stress-free holiday\n",
      "Former Wallaby Bill Young splashes $37m on Sydney hotel\n",
      "Horses may worry about their looks too: study\n",
      "Eurovision disqualifies Belarus over ‘pro-crackdown’ political lyrics\n",
      "#FreedomLobster, #FreedomPineapple: alliance driven by China’s trade wars\n",
      "Hopes for blocked Suez Canal hinge on rising tide as Ever Given’s rudder freed\n",
      "Former first ladies unite to help clean-up toxic Canberra culture\n",
      "Asian shoppers, greyer citizens and climate: three megatrends shaping NSW\n",
      "A PM who’s had his quota of mistakes should be man enough to admit them\n",
      "It’s a special place, but I don’t swim in this Sydney ocean pool paradise any more\n",
      "Turning your small apartment balcony into a serene green space\n",
      "Why this Sydney region is going through one mighty growth spurt\n",
      "From cricketers to former premiers, locals cash in on beaches boom\n",
      "Family splashes $6m on Woollahra pad as auction market passes major test\n",
      "'I'm dumbstruck': Semi sells for 650k more than the home next door\n",
      "Hamilton is both high art and thrillingly entertaining\n",
      "Julia Roberts, Ed Sheeran ride out mandatory quarantine together\n",
      "Sorry Daryl, but Hey Hey was never ‘good, clean fun’\n",
      "Megan Morton: I gave my daughter a suitcase of vintage classics for her 18th birthday\n",
      "Lights, cameras, action: The harbour city is back after a year of restrictions\n",
      "Tickled pink: The finger bun is back, and it’s no longer just a tuckshop treat\n",
      "Weekly meal planner: Budget-friendly family classics\n",
      "Roll with it: How to make this showstopping easy Easter roulade\n",
      "Letters: The big problem with regional Australia’s 10am check-out times\n",
      "Contiki tours are coming to your backyard\n",
      "More than $2000 a week: Cost of rentals soars due to car shortage\n",
      "2021 Aston Martin Vantage F1 Edition unveiled, Australian arrival confirmed\n",
      "VW's high-performance Spanish brand will finally hit Australia in 2022\n",
      "Review: Is this the perfect luxury sedan?\n",
      "The Emmy award winning series Godfather of Harlem returns April 20. Catch up on the first season now, only on Stan.\n",
      "Watch every episode of the first season of City On A Hill before the new season premieres March 29, only on Stan.\n",
      "‘Freak situations’: V’landys denies new rules to blame for injury crisis\n",
      "New faces aplenty as Rennie reveals first Wallabies squad of the year\n",
      "Pressure builds on NRL to scrap disgraced Hayne’s honours\n",
      "Browning quickest man ever in Australia, now for 100m in Tokyo\n",
      "‘I know what I’m doing’: Tszyu won’t let dad KO title dream\n",
      "Panthers star could face huge payout as sex tape scandal heads to Supreme Court\n",
      "Eels run away from Sharks in brutal Bankwest battle\n",
      "In the name of the father: The burden Mitchell Pearce has carried\n",
      "Waratahs lose fifth on the bounce as more records tumble\n",
      "Broncos break 13-match losing streak against embarrassing Bulldogs\n",
      "‘I can’t tell you the truth’: Stuart bites his tongue after costly call\n",
      "‘Buying a day’: Lions grounded in Melbourne\n",
      "Kikau the code-hopper? NRL star would be in a league of his own as a Wallaby\n",
      "Freedman tears flow as Stay Inside charges away with Golden Slipper\n",
      "Ben Hunt facing stint on sidelines after playing on with broken leg\n",
      "‘Amazing to be back’: Dream return for Buddy as super Swans crush Crows\n",
      "NRL highlights: Controversial call as Warriors earn comeback win\n",
      "Freedman tears flow as Stay Inside charges away with Golden Slipper\n",
      "Race-by-race preview and tips for the Wellington Boot meeting on Sunday\n",
      "Verry Elleegant gets Ranvet revenge on Addeybb in another slugfest\n",
      "Our Sites\n",
      "Classifieds\n",
      "The Sydney Morning Herald\n",
      "Products & Services\n"
     ]
    }
   ],
   "source": [
    "# Step 1: view the website\n",
    "# Step 2: download the website\n",
    "import requests\n",
    "\n",
    "page = requests.get(\"https://www.smh.com.au/\")\n",
    "print(page)\n",
    "\n",
    "print(page.status_code) # if this = 200 then the website was successfully downloaded (smth starting with 4 or 5 generally\n",
    "                        #  indicates an error)\n",
    "#print(page.content) # prints the page content\n",
    "\n",
    "# Step 3: parsing a page with BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#print(soup.prettify()) # prints the page content nicely formatted\n",
    "\n",
    "children = list(soup.children) # select all the elements at the top level of the page. children calls a list generator so\n",
    "                                #   need to call a list function on it\n",
    "#print(children)\n",
    "\n",
    "print([type(item) for item in list(soup.children)])\n",
    "\n",
    "html = list(soup.children)[1]\n",
    "#print(html)\n",
    "list(html.children)\n",
    "\n",
    "head = list(html.children)[0]\n",
    "head_items = list(head.children)\n",
    "#print(head_items)\n",
    "print(len(head_items))\n",
    "\n",
    "title_text = head_items[23].attrs['content']\n",
    "print(title_text)\n",
    "\n",
    "# Step 4: finding all instances of a tag at once\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#soup.find_all('p')\n",
    "\n",
    "print(soup.find_all('p')[4].get_text())\n",
    "\n",
    "# or only the first instance\n",
    "print(soup.find('p').get_text())\n",
    "\n",
    "# Steps 5-7 demonstration\n",
    "# Step 7: extract all headlines\n",
    "page = requests.get('https://www.smh.com.au/')\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#we notice that news headlines are kept in the h3 tag\n",
    "items = soup.select('h3') #get all h3 tags\n",
    "for item in items:\n",
    "    print(item.get_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: access a website\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "print(html[:60])\n",
    "\n",
    "# Part 2: extract text with 'BeautifulSoup'\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[:60])\n",
    "\n",
    "# Part 3: clean the text and transform into a NLTK object\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "print(text.concordance('gene'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RSS feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "print(llog['feed']['title'])\n",
    "print(len(llog.entries))\n",
    "\n",
    "post = llog.entries[2]\n",
    "print(post.title)\n",
    "\n",
    "content = post.content[0].value\n",
    "print(content[:70])\n",
    "\n",
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "print(word_tokenize(raw)[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: load the text file\n",
    "#f = open('document.txt')\n",
    "#raw = f.read()\n",
    "\n",
    "# Part 2: if errors appear, check all the files in the directory where IDLE is running\n",
    "#import os\n",
    "#print(os.listdir('.'))\n",
    "\n",
    "# Part 3: read in file line by line\n",
    "#f = open('document.txt', 'rU')\n",
    "\n",
    "#for line in f:\n",
    "#    print(line.strip())\n",
    "\n",
    "# Part 4: access NLTK's corpus files\n",
    "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    "raw = open(path, 'rU').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompting user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = input(\"Enter some text: \")\n",
    "\n",
    "print(\"You typed\", len(word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download webpage, strip HTML if necessary and trim to desired content\n",
    "import nltk\n",
    "from urllib import request\n",
    "\n",
    "url = \"https://www.abc.net.au/news/2021-03-19/bom-weather-forecast-dangerous-nsw-rain-floods-over-weekend/100017410\"\n",
    "\n",
    "html = request.urlopen(url).read().decode('utf-8')\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "\n",
    "raw.find(\"The Bureau of Meteorology (BOM)\")\n",
    "raw.find('the road.\"') + len('the road.\"')\n",
    "raw = raw[1127:4011]\n",
    "\n",
    "# tokenise the text, select tokens of interest (if applicable) and create a NLTK text\n",
    "tokens = nltk.wordpunct_tokenize(raw)\n",
    "print(type(tokens))\n",
    "\n",
    "#tokens = tokens[:1000]\n",
    "\n",
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "\n",
    "# normalise words and build the vocabulary\n",
    "words = [w.lower() for w in text]\n",
    "print(type(words))\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: locate the file\n",
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "\n",
    "# Part 2: open and inspect the file contents\n",
    "f = open(path, encoding = 'latin2')\n",
    "#for line in f:\n",
    "#    line = line.strip()\n",
    "#   print(line)\n",
    "    \n",
    "# Part 3: convert all non-ASCII characters into their two- and four-digit representations if contents don't display\n",
    "#           correctly or wish to see the underlying numerical values\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))\n",
    "\n",
    "# Part 4-6\n",
    "print(ord('ń')) # locate the integer ordinal of a character\n",
    "\n",
    "print(hex(324)) # define strings with their appropriate escape sequence\n",
    "nacute = '\\u0144'\n",
    "print(nacute)\n",
    "\n",
    "print(nacute.encode('utf8')) # determine how a character is represented as a sequence of bites inside a text file\n",
    "\n",
    "# Part 7: inspect the properties of unicode characters\n",
    "import unicodedata\n",
    "\n",
    "lines = open(path, encoding = 'latin2').readlines()\n",
    "line = lines[2]\n",
    "print(line.encode('unicode_escape'))\n",
    "\n",
    "for c in line:\n",
    "    if ord(c) > 127:\n",
    "        print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using basic meta-characters\n",
    "import re\n",
    "\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "\n",
    "# search for words ending in 'ed'\n",
    "print([w for w in wordlist if re.search('ed$', w)][:20])\n",
    "\n",
    "# count the number of occurences of a word\n",
    "print(sum(1 for w in text if re.search('^e-?mail$', w)))\n",
    "\n",
    "## Useful applications of regular expressions\n",
    "# 1. Extracting word pieces\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "# re.findall() finds all non-overlapping matches of a given regex\n",
    "                  for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.most_common(12)\n",
    "\n",
    "# 2. Finding word stems\n",
    "# function to strip anything looking like a suffix from a word\n",
    "def stem_word(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "# build a disjunction of all suffixes\n",
    "print(re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing'))\n",
    "print(re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')) # specifies scope of disjunction but not\n",
    "                                                                             #  material to be output\n",
    "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')) # split word into stem and suffix\n",
    "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes'))  # but there's a problem...\n",
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')) # need to use the non-greedy version of *\n",
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')) # works even with a non-existent suffix when ?\n",
    "                                                                             #  included at end of second parentheses\n",
    "\n",
    "# application to a whole text\n",
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. \n",
    "         Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "print([stem(t) for t in tokens])\n",
    "\n",
    "## Lemmatisation\n",
    "# will only remove affixes if the word is in its dictionary\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])\n",
    "\n",
    "## Regular expression tokeniser\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "   | \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "   | \\.\\.\\.            # ellipsis\n",
    "   | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\n",
    " '''\n",
    "\n",
    "print(nltk.regexp_tokenize(text, pattern))\n",
    "\n",
    "## Sentence segmentation\n",
    "print(len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents()))\n",
    "\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[79:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning to Classify Text\n",
    "\n",
    "##### Supervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gender identification\n",
    "# Part 1: define a function to build a dictionary containing relevant information about a given name\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "gender_features('Shrek')\n",
    "\n",
    "# Part 2: prepare a list of examples and corresponding class labels\n",
    "from nltk.corpus import names\n",
    "labelled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "import random\n",
    "random.shuffle(labelled_names)\n",
    "\n",
    "# Part 3: use a feature extractor to process the data and split it into a training and test set\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labelled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Part 4: test it on some names in the test set\n",
    "print(classifier.classify(gender_features('Neo')))\n",
    "print(classifier.classify(gender_features('Trinity')))\n",
    "\n",
    "# Part 5: evaluate it on a larger test set\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# Part 6: determine which features the classifier found most useful for distinguishing gender\n",
    "print(classifier.show_most_informative_features(5))\n",
    "\n",
    "## Document classification\n",
    "# Part 1: choose a corpus\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Part 2: define a feature extractor that checks whether one of the 2,000 most-frequent words from the corpus are in a\n",
    "#           particular document\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "dict = document_features(movie_reviews.words('pos/cv957_8737.txt'))\n",
    "for key in list(dict)[:10]:\n",
    "    print(key, dict[key])\n",
    "\n",
    "# Part 3: use feature extractor to train classifier to label new movie reviews\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: create training and test sets by randomly assigning sentences from a data source reflecting a single genre\n",
    "# training and test sets will be very similar, and will not be able to confidently generalise results to other genres\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "random.shuffle(tagged_sents)\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_set, test_set = tagged_sents[size:], tagged_sents[:size]\n",
    "\n",
    "# Option 2: create training and test sets from different documents \n",
    "file_ids = brown.fileids(categories='news')\n",
    "size = int(len(file_ids) * 0.1)\n",
    "train_set = brown.tagged_sents(file_ids[size:])\n",
    "test_set = brown.tagged_sents(file_ids[:size])\n",
    "\n",
    "# Option 3: create test set from documents less-closely related to those in training set\n",
    "# more stringent evaluation of classifier. If it performs well here, confident that classifier will perform well on data\n",
    "#   very different to that it was trained on \n",
    "train_set = brown.tagged_sents(categories='news')\n",
    "test_set = brown.tagged_sents(categories='fiction')\n",
    "\n",
    "# Accuracy - simplest metric used to evaluate a classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
