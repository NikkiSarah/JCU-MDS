{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3 Topic 3: Text Processing with NLTK Part 2\n",
    "\n",
    "#### Accessing Text From the Web and From Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Electronic books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "1176967\n",
      "﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\n",
      "<class 'list'>\n",
      "257085\n",
      "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']\n",
      "<class 'nltk.text.Text'>\n",
      "['I', 'CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in', 'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in', 'which', 'he', 'lodged', 'in', 'S.', 'Place', 'and', 'walked', 'slowly', ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K.', 'bridge']\n",
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n",
      "None\n",
      "5336\n",
      "-1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Part 1: access text 2554 (\"Crime and Punishment\")\n",
    "# the read() process can take a couple of seconds as it's downloading a large book. If an internet proxy is being used that\n",
    "# isn't correctly detected by Python, the proxy may need to be specified manually before using urlopen()\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "# proxies = {'http': 'http://www.someproxy.com:3128'}\n",
    "# request.ProxyHandler(proxies)\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "\n",
    "print(type(raw))\n",
    "print(len(raw))\n",
    "print(raw[:75])\n",
    "\n",
    "# Part 2: text tokenisation\n",
    "tokens = word_tokenize(raw)\n",
    "print(type(tokens))\n",
    "print(len(tokens))\n",
    "print(tokens[:10])\n",
    "\n",
    "# Part 3: create an NLTK text from this list\n",
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "print(text[1024:1062])\n",
    "print(text.collocations())\n",
    "\n",
    "# Part 4: locate unique strings marking the beginning and end before trimming\n",
    "print(raw.find(\"PART I\"))\n",
    "print(raw.rfind(\"End of Project Gutenberg's Crime\")) # reverse find\n",
    "\n",
    "raw = raw[5336:-1]\n",
    "print(raw.find(\"PART I\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\n",
      "['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', \"'to\", 'die', 'out', 'in', '200', \"years'\", 'NEWS', 'SPORT', 'WEATHER', 'WORLD', 'SERVICE', 'A-Z', 'INDEX', 'SEARCH', 'You', 'are', 'in', ':', 'Health', 'News', 'Front', 'Page', 'Africa', 'Americas', 'Asia-Pacific', 'Europe', 'Middle', 'East', 'South', 'Asia', 'UK', 'Business', 'Entertainment', 'Science/Nature', 'Technology', 'Health', 'Medical', 'notes', '--', '--', '--', '--', '--', '--', '-', 'Talking', 'Point', '--', '--', '--', '--', '--', '--', '-']\n",
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Part 1: access a website\n",
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "print(html[:60])\n",
    "\n",
    "# Part 2: extract text with 'BeautifulSoup'\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[:60])\n",
    "\n",
    "# Part 3: clean the text and transform into a NLTK object\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "\n",
    "print(text.concordance('gene'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RSS feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Log\n",
      "13\n",
      "Equal representation in the halls of quackery\n",
      "<p><a href=\"https://www.smbc-comics.com/comic/pp\" rel=\"noopener\" targe\n",
      "['Today', \"'s\", 'SMBC', 'starts', 'this', 'way', ':', 'The', 'rant', 'continues', 'at', 'some', 'length', '—', 'I', 'think', 'my', 'favorite', 'parts', 'are', 'the', 'digital', 'chakra-detoxification', 'algorithms', 'and', 'the', 'Turing-complete', 'microbiome', '(', 'with', 'its', 'obvious', 'connections', 'to', 'nanotechnology', ',', 'edge', 'computing', ',', 'message-passing', 'via', 'mRNA', ',', 'etc', '.', ')', ':', 'The', 'mouseover', 'title', '[', 'link', 'added', ']', ':', '``', 'As', 'I', 'post', 'this']\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "print(llog['feed']['title'])\n",
    "print(len(llog.entries))\n",
    "\n",
    "post = llog.entries[2]\n",
    "print(post.title)\n",
    "\n",
    "content = post.content[0].value\n",
    "print(content[:70])\n",
    "\n",
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "print(word_tokenize(raw)[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-7f18d928af09>:17: DeprecationWarning: 'U' mode is deprecated\n",
      "  raw = open(path, 'rU').read()\n"
     ]
    }
   ],
   "source": [
    "# Part 1: load the text file\n",
    "#f = open('document.txt')\n",
    "#raw = f.read()\n",
    "\n",
    "# Part 2: if errors appear, check all the files in the directory where IDLE is running\n",
    "#import os\n",
    "#print(os.listdir('.'))\n",
    "\n",
    "# Part 3: read in file line by line\n",
    "#f = open('document.txt', 'rU')\n",
    "\n",
    "#for line in f:\n",
    "#    print(line.strip())\n",
    "\n",
    "# Part 4: access NLTK's corpus files\n",
    "path = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')\n",
    "raw = open(path, 'rU').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompting user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter some text: helo\n",
      "You typed 1 words.\n"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter some text: \")\n",
    "\n",
    "print(\"You typed\", len(word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download webpage, strip HTML if necessary and trim to desired content\n",
    "import nltk\n",
    "from urllib import request\n",
    "\n",
    "url = \"https://www.abc.net.au/news/2021-03-19/bom-weather-forecast-dangerous-nsw-rain-floods-over-weekend/100017410\"\n",
    "\n",
    "html = request.urlopen(url).read().decode('utf-8')\n",
    "raw = BeautifulSoup(html).get_text()\n",
    "\n",
    "raw.find(\"The Bureau of Meteorology (BOM)\")\n",
    "raw.find('the road.\"') + len('the road.\"')\n",
    "raw = raw[1127:4011]\n",
    "\n",
    "# tokenise the text, select tokens of interest (if applicable) and create a NLTK text\n",
    "tokens = nltk.wordpunct_tokenize(raw)\n",
    "print(type(tokens))\n",
    "\n",
    "#tokens = tokens[:1000]\n",
    "\n",
    "text = nltk.Text(tokens)\n",
    "print(type(text))\n",
    "\n",
    "# normalise words and build the vocabulary\n",
    "words = [w.lower() for w in text]\n",
    "print(type(words))\n",
    "\n",
    "vocab = sorted(set(words))\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Pruska Biblioteka Pa\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\u0105'\n",
      "b'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y'\n",
      "b'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\u0142y do Biblioteki'\n",
      "b'Jagiello\\\\u0144skiej w Krakowie, obejmuj\\\\u0105 ponad 500 tys. zabytkowych'\n",
      "b'archiwali\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.'\n",
      "324\n",
      "0x144\n",
      "ń\n",
      "b'\\xc5\\x84'\n",
      "b'Niemc\\\\xf3w pod koniec II wojny \\\\u015bwiatowej na Dolny \\\\u015al\\\\u0105sk, zosta\\\\u0142y\\\\n'\n",
      "b'\\xc3\\xb3' U+00f3 LATIN SMALL LETTER O WITH ACUTE\n",
      "b'\\xc5\\x9b' U+015b LATIN SMALL LETTER S WITH ACUTE\n",
      "b'\\xc5\\x9a' U+015a LATIN CAPITAL LETTER S WITH ACUTE\n",
      "b'\\xc4\\x85' U+0105 LATIN SMALL LETTER A WITH OGONEK\n",
      "b'\\xc5\\x82' U+0142 LATIN SMALL LETTER L WITH STROKE\n"
     ]
    }
   ],
   "source": [
    "# Part 1: locate the file\n",
    "path = nltk.data.find('corpora/unicode_samples/polish-lat2.txt')\n",
    "\n",
    "# Part 2: open and inspect the file contents\n",
    "f = open(path, encoding = 'latin2')\n",
    "#for line in f:\n",
    "#    line = line.strip()\n",
    "#   print(line)\n",
    "    \n",
    "# Part 3: convert all non-ASCII characters into their two- and four-digit representations if contents don't display\n",
    "#           correctly or wish to see the underlying numerical values\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    print(line.encode('unicode_escape'))\n",
    "\n",
    "# Part 4-6\n",
    "print(ord('ń')) # locate the integer ordinal of a character\n",
    "\n",
    "print(hex(324)) # define strings with their appropriate escape sequence\n",
    "nacute = '\\u0144'\n",
    "print(nacute)\n",
    "\n",
    "print(nacute.encode('utf8')) # determine how a character is represented as a sequence of bites inside a text file\n",
    "\n",
    "# Part 7: inspect the properties of unicode characters\n",
    "import unicodedata\n",
    "\n",
    "lines = open(path, encoding = 'latin2').readlines()\n",
    "line = lines[2]\n",
    "print(line.encode('unicode_escape'))\n",
    "\n",
    "for c in line:\n",
    "    if ord(c) > 127:\n",
    "        print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'abridged', 'abscessed', 'absconded', 'absorbed', 'abstracted', 'abstricted', 'accelerated', 'accepted', 'accidented', 'accoladed', 'accolated', 'accomplished', 'accosted']\n",
      "0\n",
      "['ing']\n",
      "['processing']\n",
      "[('process', 'ing')]\n",
      "[('processe', 's')]\n",
      "[('process', 'es')]\n",
      "[('language', '')]\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "[('', '', ''), ('A.', '', ''), ('', '-print', ''), ('', '', ''), ('', '', '.40'), ('', '', '')]\n",
      "20.250994070456922\n",
      "['\"Nonsense!\"',\n",
      " 'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\n'\n",
      " 'railway trains look so sad and tired, so very sad and tired?',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\n'\n",
      " 'is because they know that whatever place they have taken a ticket\\n'\n",
      " 'for that place they will reach.',\n",
      " 'It is because after they have\\n'\n",
      " 'passed Sloane Square they know that the next station must be\\n'\n",
      " 'Victoria, and nothing but Victoria.',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\n'\n",
      " 'their eyes like stars and their souls again in Eden, if the next\\n'\n",
      " 'station were unaccountably Baker Street!\"',\n",
      " '\"It is you who are unpoetical,\" replied the poet Syme.']\n"
     ]
    }
   ],
   "source": [
    "## Using basic meta-characters\n",
    "import re\n",
    "\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "\n",
    "# search for words ending in 'ed'\n",
    "print([w for w in wordlist if re.search('ed$', w)][:20])\n",
    "\n",
    "# count the number of occurences of a word\n",
    "print(sum(1 for w in text if re.search('^e-?mail$', w)))\n",
    "\n",
    "## Useful applications of regular expressions\n",
    "# 1. Extracting word pieces\n",
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj\n",
    "# re.findall() finds all non-overlapping matches of a given regex\n",
    "                  for vs in re.findall(r'[aeiou]{2,}', word))\n",
    "fd.most_common(12)\n",
    "\n",
    "# 2. Finding word stems\n",
    "# function to strip anything looking like a suffix from a word\n",
    "def stem_word(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "# build a disjunction of all suffixes\n",
    "print(re.findall(r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing'))\n",
    "print(re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')) # specifies scope of disjunction but not\n",
    "                                                                             #  material to be output\n",
    "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')) # split word into stem and suffix\n",
    "print(re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes'))  # but there's a problem...\n",
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')) # need to use the non-greedy version of *\n",
    "print(re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')) # works even with a non-existent suffix when ?\n",
    "                                                                             #  included at end of second parentheses\n",
    "\n",
    "# application to a whole text\n",
    "def stem(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. \n",
    "         Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "print([stem(t) for t in tokens])\n",
    "\n",
    "## Lemmatisation\n",
    "# will only remove affixes if the word is in its dictionary\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])\n",
    "\n",
    "## Regular expression tokeniser\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "pattern = r'''(?x)     # set flag to allow verbose regexps\n",
    "     ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "   | \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "   | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "   | \\.\\.\\.            # ellipsis\n",
    "   | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\n",
    " '''\n",
    "\n",
    "print(nltk.regexp_tokenize(text, pattern))\n",
    "\n",
    "## Sentence segmentation\n",
    "print(len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents()))\n",
    "\n",
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[79:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning to Classify Text\n",
    "\n",
    "##### Supervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "female\n",
      "0.808\n",
      "Most Informative Features\n",
      "             last_letter = 'k'              male : female =     44.5 : 1.0\n",
      "             last_letter = 'a'            female : male   =     35.5 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.1 : 1.0\n",
      "             last_letter = 'p'              male : female =     12.4 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.9 : 1.0\n",
      "None\n",
      "contains(,) True\n",
      "contains(the) True\n",
      "contains(.) True\n",
      "contains(a) True\n",
      "contains(and) True\n",
      "contains(of) True\n",
      "contains(to) True\n",
      "contains(') True\n",
      "contains(is) True\n",
      "contains(in) True\n",
      "0.84\n",
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     11.6 : 1.0\n",
      "         contains(mulan) = True              pos : neg    =      8.4 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      7.4 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      6.4 : 1.0\n",
      "         contains(damon) = True              pos : neg    =      6.1 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Gender identification\n",
    "# Part 1: define a function to build a dictionary containing relevant information about a given name\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "gender_features('Shrek')\n",
    "\n",
    "# Part 2: prepare a list of examples and corresponding class labels\n",
    "from nltk.corpus import names\n",
    "labelled_names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "                 [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "import random\n",
    "random.shuffle(labelled_names)\n",
    "\n",
    "# Part 3: use a feature extractor to process the data and split it into a training and test set\n",
    "featuresets = [(gender_features(n), gender) for (n, gender) in labelled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Part 4: test it on some names in the test set\n",
    "print(classifier.classify(gender_features('Neo')))\n",
    "print(classifier.classify(gender_features('Trinity')))\n",
    "\n",
    "# Part 5: evaluate it on a larger test set\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# Part 6: determine which features the classifier found most useful for distinguishing gender\n",
    "print(classifier.show_most_informative_features(5))\n",
    "\n",
    "## Document classification\n",
    "# Part 1: choose a corpus\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Part 2: define a feature extractor that checks whether one of the 2,000 most-frequent words from the corpus are in a\n",
    "#           particular document\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "dict = document_features(movie_reviews.words('pos/cv957_8737.txt'))\n",
    "for key in list(dict)[:10]:\n",
    "    print(key, dict[key])\n",
    "\n",
    "# Part 3: use feature extractor to train classifier to label new movie reviews\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: create training and test sets by randomly assigning sentences from a data source reflecting a single genre\n",
    "# training and test sets will be very similar, and will not be able to confidently generalise results to other genres\n",
    "import random\n",
    "from nltk.corpus import brown\n",
    "\n",
    "tagged_sents = list(brown.tagged_sents(categories='news'))\n",
    "random.shuffle(tagged_sents)\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_set, test_set = tagged_sents[size:], tagged_sents[:size]\n",
    "\n",
    "# Option 2: create training and test sets from different documents \n",
    "file_ids = brown.fileids(categories='news')\n",
    "size = int(len(file_ids) * 0.1)\n",
    "train_set = brown.tagged_sents(file_ids[size:])\n",
    "test_set = brown.tagged_sents(file_ids[:size])\n",
    "\n",
    "# Option 3: create test set from documents less-closely related to those in training set\n",
    "# more stringent evaluation of classifier. If it performs well here, confident that classifier will perform well on data\n",
    "#   very different to that it was trained on \n",
    "train_set = brown.tagged_sents(categories='news')\n",
    "test_set = brown.tagged_sents(categories='fiction')\n",
    "\n",
    "# Accuracy - simplest metric used to evaluate a classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonD",
   "language": "python",
   "name": "pythond"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
