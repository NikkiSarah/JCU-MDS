{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name = \"Nikki Fitzherbert\"\n",
    "student_id = \"13848336\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This week we will look at working with text as data, how to extract features from text and the use of a clustering algorithm.\n",
    "\n",
    "We will take some samples of texts and look at how to extract a fixed set of features from each text to use in clustering.   We'll then look at how to measure the similarity or distance between two texts. Finally we'll look at the KMeans clustering algorithm.\n",
    "\n",
    "New concepts this week:\n",
    "- using **feature extraction** methods to create features from texts\n",
    "- **sparse arrays** are used to store arrays where many of the values will be zero\n",
    "- comparing the similarity of two samples using a **distance metric**\n",
    "- the **kmeans clustering** algorithm\n",
    "\n",
    "## Finding Text Data\n",
    "\n",
    "The example this week is derived from [this example](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html) in the sklearn documentation.\n",
    "\n",
    "We will use some data from sklearn, this is the [20 newsgroups dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#newsgroups) containing messages from the old Usenet discussion boards.   We select just four of the groups giving us messages on four topics.  We choose two that are probably quite close together (atheism and religion) and two that should be quite different.\n",
    "\n",
    "The result is an sklearn dataset, the actual data is available as dataset.data, the newsgroup names are in dataset.target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3387"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "len(dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Re: who are we to judge, Bobby?\n",
      "Lines: 38\n",
      "Organization: Walla Walla College\n",
      "Lines: 38\n",
      "\n",
      "In article <1993Apr14.213356.22176@ultb.isc.rit.edu> snm6394@ultb.isc.rit.edu (S.N. Mozumder ) writes:\n",
      ">From: snm6394@ultb.isc.rit.edu (S.N. Mozumder )\n",
      ">Subject: Re: who are we to judge, Bobby?\n",
      ">Date: Wed, 14 Apr 1993 21:33:56 GMT\n",
      ">In article <healta.56.734556346@saturn.wwc.edu> healta@saturn.wwc.edu (TAMMY R HEALY) writes:\n",
      ">>Bobby,\n",
      ">>\n",
      ">>I would like to take the liberty to quote from a Christian writer named \n",
      ">>Ellen G. White.  I hope that what she said will help you to edit your \n",
      ">>remarks in this group in the future.\n",
      ">>\n",
      ">>\"Do not set yourself as a standard.  Do not make your opinions, your views \n",
      ">>of duty, your interpretations of scripture, a criterion for others and in \n",
      ">>your heart condemn them if they do not come up to your ideal.\"\n",
      ">>                         Thoughts Fromthe Mount of Blessing p. 124\n",
      ">>\n",
      ">>I hope quoting this doesn't make the atheists gag, but I think Ellen White \n",
      ">>put it better than I could.\n",
      ">> \n",
      ">>Tammy\n",
      ">\n",
      ">Point?\n",
      ">\n",
      ">Peace,\n",
      ">\n",
      ">Bobby Mozumder\n",
      ">\n",
      "My point is that you set up your views as the only way to believe.  Saying \n",
      "that all eveil in this world is caused by atheism is ridiculous and \n",
      "counterproductive to dialogue in this newsgroups.  I see in your posts a \n",
      "spirit of condemnation of the atheists in this newsgroup bacause they don'\n",
      "t believe exactly as you do.  If you're here to try to convert the atheists \n",
      "here, you're failing miserably.  Who wants to be in position of constantly \n",
      "defending themselves agaist insulting attacks, like you seem to like to do?!\n",
      "I'm sorry you're so blind that you didn't get the messgae in the quote, \n",
      "everyone else has seemed to.\n",
      "\n",
      "Tammy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can look at the first message in the data\n",
    "print(dataset.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "We can't work directly with text as data - we need some kind of numerical or categorical features to use in the algorithms we're working with.  Text has a variable number of words per sample, we need a fixed set of features.\n",
    "\n",
    "A very common feature type for text treats each sample as a *bag of words* and just records how often each word is present in the text.  Each word becomes a feature, the value is the count of how many times it occurs in the sample.  Of course, there will be thousands of words in general, so we just choose the N most frequent words as features.  \n",
    "\n",
    "The idea is that if a particular word occurs a lot in two texts, they might be similar. If the same pattern \n",
    "of words is frequent in both, even more so.  However, some words are very frequent in all texts - and, of, \n",
    "is etc - they don't tell you much about what the text is saying; it is common to remove these common \n",
    "words, generally known as *stop words*, before you do any feature extraction.\n",
    "\n",
    "SKLearn has a collection of [text feature extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) methods that we can make use of. We'll use the simplest of these, [CountVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage) which just counts the number of times a word occurs in the text.  We pass it a parameter that defines the maximum number of features (words) to use and the name of the stop word list.  \n",
    "\n",
    "Once we've made a vectorizer, we can use the *fit_transform* method to apply it to a set of data. \n",
    "In this first example we will just compute 10 features, just to make it easier to look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(max_features=10, stop_words='english')\n",
    "X = count_vec.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result *X_count* is a SciPy [sparse matrix](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html).  \n",
    "Many of the feature values will be zero if the given word does not occur in the text. To store all of these\n",
    "zeros would be very wasteful of memory, so we use a *sparse matrix* which uses methods to only store\n",
    "the data that is non-zero.  The SciPy sparse matrix classes support some of the matrix methods that you can use\n",
    "on regular Numpy arrays or Pandas dataframes, but not all.  \n",
    "\n",
    "In the example below we use the *getrow* method to get a single row and the *toarray* method to convert this to a regular numpy array.  \n",
    "\n",
    "First, we can look at the words that have been selected as features via the *feature_names* method on the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article',\n",
       " 'com',\n",
       " 'don',\n",
       " 'edu',\n",
       " 'god',\n",
       " 'lines',\n",
       " 'organization',\n",
       " 'space',\n",
       " 'subject',\n",
       " 'writes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we only chose 10 features so they aren't likely to be very good at characterising the texts.  You might\n",
    "also notice that we have 'words' like *com* and *edu*, probably from email addresses and *don* which is\n",
    "probably from *don't*.  The question of what is a word is not a simple one.\n",
    "\n",
    "## Measuring Similarity\n",
    "\n",
    "We now have a fixed size feature set for each text - the frequency of ten words.  We can look at the features\n",
    "that have been computed for the first text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 1, 6, 0, 2, 1, 0, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.getrow(0).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this means that the word *article* appears twice in the text, *edu* appears six times and *com* and *god* do not\n",
    "appear at all.  \n",
    "\n",
    "If we want to measure the **similarity** of this text with another, we can compare their feature vectors.  If we\n",
    "were dealing with points on a plane in a geometry problem, we could work out the **distance** between\n",
    "the points using Pythagoras Theorem. Two points that were very close could be said to be very similar.  This is \n",
    "known as the **Euclidean distance** metric and in fact, we can use it for this problem too. \n",
    "\n",
    "The Euclidean distance is defined as the square root of the sum of the squares of the differences between each \n",
    "pair of feature values:\n",
    "\n",
    "\\begin{equation*}\n",
    "distance = \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2}\n",
    "\\end{equation*}\n",
    "\n",
    "Here's an example of computing the distance between the first two rows of the dataset.  I've done it explicitly\n",
    "with raw vector arithmetic and then using the SciPy *euclidean* function as a check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between articles 0 and 1: 4.58257569495584 4.58257569495584\n"
     ]
    }
   ],
   "source": [
    "a1 = count_vec.transform([dataset.data[0]]).toarray()[0]\n",
    "a2 = X.getrow(1).toarray()[0]\n",
    "\n",
    "# import the scipy euclidean function as a check\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "d1= np.sqrt((np.square(a1-a2)).sum())\n",
    "d2= euclidean(a1, a2) \n",
    "print(\"Distance between articles 0 and 1:\", d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this distance isn't a physical distance in metres,m it has no units, we just know that if it \n",
    "is bigger, the articles are more different in their feature sets.\n",
    "\n",
    "We can use this to look through the data and find the most similar article to a given target text.  The function \n",
    "I've written below calculates the euclidean distance between a given target article and every other article\n",
    "in the dataset.  It remembers the article with the smallest distance and returns it's index.\n",
    "\n",
    "I've tested this using the vectorizer I made above (*count_vec*) to find the closest article\n",
    "to the first one in the datast (note that I've passed dataset.data[:1] to the function so that I \n",
    "don't just find the first article). The result is not very similar - we're only using 10 word features\n",
    "after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest article is  1210\n",
      "From: andreasa@dhhalden.no (ANDREAS ARFF)\n",
      "Subject: Re: Newsgroup Split\n",
      "Lines: 41\n",
      "Nntp-Posting-Host: pc137\n",
      "Organization: Ostfold College\n",
      "\n",
      "In article <NERONE.93Apr20085951@sylvester.cc.utexas.edu> nerone@ccwf.cc.utexas.edu (Michael Nerone) writes:\n",
      ">From: nerone@ccwf.cc.utexas.edu (Michael Nerone)\n",
      ">Subject: Re: Newsgroup Split\n",
      ">Date: 20 Apr 93 08:59:51\n",
      ">In article <1quvdoINN3e7@srvr1.engin.umich.edu>, tdawson@engin.umich.edu (Chris Herringshaw) writes:\n",
      ">\n",
      ">  CH> Concerning the proposed newsgroup split, I personally am not in\n",
      ">  CH> favor of doing this.  I learn an awful lot about all aspects of\n",
      ">  CH> graphics by reading this group, from code to hardware to\n",
      ">  CH> algorithms.  I just think making 5 different groups out of this\n",
      ">  CH> is a wate, and will only result in a few posts a week per group.\n",
      ">  CH> I kind of like the convenience of having one big forum for\n",
      ">  CH> discussing all aspects of graphics.  Anyone else feel this way?\n",
      ">  CH> Just curious.\n",
      ">\n",
      ">I must agree.  There is a dizzying number of c.s.amiga.* newsgroups\n",
      ">already.  In addition, there are very few issues which fall cleanly\n",
      ">into one of these categories.\n",
      ">\n",
      ">Also, it is readily observable that the current spectrum of amiga\n",
      ">groups is already plagued with mega-crossposting; thus the group-split\n",
      ">would not, in all likelihood, bring about a more structured\n",
      ">environment.\n",
      ">\n",
      ">--\n",
      ">   /~~~~~~~~~~~~~~~~~~~\\/~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\n",
      ">  /    Michael Nerone   \\\"I shall do so with my customary lack of tact; and\\\n",
      "> /   Internet Address:   \\since you have asked for this, you will be obliged\\\n",
      ">/nerone@ccwf.cc.utexas.edu\\to pardon it.\"-Sagredo, fictional char of Galileo.\\\n",
      "\n",
      "\n",
      "Maybe I should point out that we are not talking about c.s.amiga.*.\n",
      "Only comp.graphics.\n",
      "\n",
      "Arff\n",
      "\"Also for the not religous confessor, there is a mystery of higher values,\n",
      "who's birth mankind - to the last - builds upon. They are indisputible. And \n",
      "often disregarded. Seldom you hear them beeing prized, as seldom as you hear \n",
      "a seeing man prizeing what he sees.\" Per Lagerkvist, The Fist \n",
      "(Free translation from Swedish)\n",
      "              --Andreas Arff  andreasa@dhhalden.no--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_closest(dataset, target, vectorizer):\n",
    "    \"\"\"Find the most similar article in dataset to target using \n",
    "    the given vectorizer to extract feature vectors\n",
    "    Returns the index of the most similar article\"\"\"\n",
    "    \n",
    "    a1 = vectorizer.transform([target]).toarray()[0]\n",
    "    X = vectorizer.transform(dataset)\n",
    "    \n",
    "    best = 0\n",
    "    best_dist = 9999\n",
    "    for i in range(X.shape[0]):\n",
    "        a2 = X.getrow(i).toarray()[0]\n",
    "        dist = euclidean(a1, a2)\n",
    "        if dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best = i\n",
    "    return best\n",
    "\n",
    "best = find_closest(dataset.data[1:], dataset.data[0], count_vec)\n",
    "\n",
    "print(\"Closest article is \", best)\n",
    "print(dataset.data[1:][best])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint** Repeat the analysis I did above but use a larger number of features - say 200.  Do you get a\n",
    "result that is more similar to the target article? (Hint: there is another article that directly quotes\n",
    "this one, that should be very similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '12', '14', '15', '16', '1993', '20', '24', '3d', 'ac', 'access', 'actually', 'argument', 'article', 'atheism', 'atheists', 'au', 'available', 'based', 'believe']\n",
      "[[0 0 1 0 0 1 0 0 0 0 0 0 0 2 1 3 0 0 0 2 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 2 0 0 0 0 0 0 0\n",
      "  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 1 0 0 1 2 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# make a new vectoriser with more features and repeat the analysis...\n",
    "count_vec2 = CountVectorizer(max_features=200, stop_words='english')\n",
    "X = count_vec2.fit_transform(dataset.data)\n",
    "\n",
    "# look at first 20 words chosen...\n",
    "print(count_vec2.get_feature_names()[:20])\n",
    "\n",
    "#... and see how often they appear in the first text\n",
    "print(X.getrow(0).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the most similar article is 3157 and the text for that article is printed below:\n",
      "From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Re: who are we to judge, Bobby?\n",
      "Lines: 31\n",
      "Organization: Walla Walla College\n",
      "Lines: 31\n",
      "\n",
      "In article <kmr4.1572.734847158@po.CWRU.edu> kmr4@po.CWRU.edu (Keith M. Ryan) writes:\n",
      ">From: kmr4@po.CWRU.edu (Keith M. Ryan)\n",
      ">Subject: Re: who are we to judge, Bobby?\n",
      ">Date: Thu, 15 Apr 1993 04:12:38 GMT\n",
      ">\n",
      ">(S.N. Mozumder ) writes:\n",
      ">>(TAMMY R HEALY) writes:\n",
      ">>>I would like to take the liberty to quote from a Christian writer named \n",
      ">>>Ellen G. White.  I hope that what she said will help you to edit your \n",
      ">>>remarks in this group in the future.\n",
      ">>>\n",
      ">>>\"Do not set yourself as a standard.  Do not make your opinions, your views \n",
      ">>>of duty, your interpretations of scripture, a criterion for others and in \n",
      ">>>your heart condemn them if they do not come up to your ideal.\"\n",
      ">>>                         Thoughts Fromthe Mount of Blessing p. 124\n",
      ">>\n",
      ">>Point?\n",
      ">\n",
      ">\tPoint: you have taken it upon yourself to judge others; when only \n",
      ">God is the true judge.\n",
      ">\n",
      ">---\n",
      ">\n",
      ">   Only when the Sun starts to orbit the Earth will I accept the Bible. \n",
      ">        \n",
      ">\n",
      "I agree totally with you!  Amen!  You stated it better and in less world \n",
      "than I did.\n",
      "\n",
      "Tammy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the 'find_closest' function defined above with the expanded vectoriser\n",
    "best2 = find_closest(dataset.data[1:], dataset.data[0], count_vec2)\n",
    "\n",
    "print(f\"The index of the most similar article is {best2} and the text for that article is printed below:\")\n",
    "print(dataset.data[1:][best2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Clustering\n",
    "\n",
    "Finally we look at the [KMeans clustering algorithm](http://scikit-learn.org/stable/modules/clustering.html#k-means).  This makes use of the distance metric like the one\n",
    "we've used above.  KMeans tries to find a given number of clusters in the data. It does this by grouping\n",
    "together the samples that are closest to one another using the distance metric.\n",
    "\n",
    "KMeans starts by choosing K points (K is the number of clusters) somewhere in the space. These\n",
    "are the initial cluster centres. It then assigns\n",
    "each sample to one cluster based on which cluster centre it is closest too.   Once all points are\n",
    "in a cluster, the cluster centre is re-computed and the process is repeated.  This continues until\n",
    "there is no (or little) change to the centroids or until some maximum number of iterations.  \n",
    "\n",
    "In this example we ask the algorithm to look for 4 clusters in our data, the verbose flag will \n",
    "show the number of iterations as it runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 607173.0\n",
      "Iteration 1, inertia 498535.88602460123\n",
      "Iteration 2, inertia 492094.45821227104\n",
      "Iteration 3, inertia 489155.5887515047\n",
      "Iteration 4, inertia 486739.4551678453\n",
      "Iteration 5, inertia 485598.15711688984\n",
      "Iteration 6, inertia 485010.1803354205\n",
      "Iteration 7, inertia 484844.45952904725\n",
      "Iteration 8, inertia 484764.8039701195\n",
      "Iteration 9, inertia 484697.4813506927\n",
      "Iteration 10, inertia 484674.72491218866\n",
      "Iteration 11, inertia 484672.3718347494\n",
      "Converged at iteration 11: strict convergence.\n"
     ]
    }
   ],
   "source": [
    "# add seed to ensure results are consistent over multiple runs of the algorithm\n",
    "km = KMeans(n_clusters=4, init='k-means++', max_iter=100, n_init=1, verbose=True, random_state=2021)\n",
    "X_count = X\n",
    "km.fit(X_count)\n",
    "labels = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*km* is now the result of clustering, *km.labels_* are the labels assigned to each sample, they are just numbers 0..3 since the algorithm doesn't know what the true labels were.   \n",
    "\n",
    "This is an example of an **Unsupervised Learning** algorithm.  We didn't tell it what the true answer\n",
    "was, we just asked it to look for a given number of clusters in the data.  \n",
    "\n",
    "To evaluate the result we can use the [SKLearn metrics](http://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure) module.  Here we compute:\n",
    "\n",
    "- homogeneity -- larger if each cluster contains members of a single class\n",
    "- completeness -- larger if all samples from a single class are in the same cluster\n",
    "- v-measure -- is the harmonic mean of the homogeneity and completeness\n",
    "\n",
    "Ideally, these metrics would be close to 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.009\n",
      "Completeness: 0.076\n",
      "V-measure: 0.016\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "As an extension exercise, repeat the KMeans clustering exercise but use an alternate feature vector. \n",
    "The [TfidfVectorizer](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting) (from sklearn.feature_extraction.text import TfidfVectorizer) uses a measure \n",
    "tf-idf that tries to measure how characteristic a word is in a text.  Words that are usually infrequent\n",
    "but occur many times in a text will have a higher score.   Use a much higer number of features (say 1000) and \n",
    "see if you can get a better set of evaluation scores than in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 5980.036689926416\n",
      "Iteration 1, inertia 3142.405017245314\n",
      "Iteration 2, inertia 3120.988890632172\n",
      "Iteration 3, inertia 3111.9771846670974\n",
      "Iteration 4, inertia 3109.118415157848\n",
      "Iteration 5, inertia 3106.9536569119655\n",
      "Iteration 6, inertia 3106.300125743249\n",
      "Iteration 7, inertia 3105.9986765978356\n",
      "Iteration 8, inertia 3105.8654083959523\n",
      "Iteration 9, inertia 3105.7338642396257\n",
      "Iteration 10, inertia 3105.6011265332145\n",
      "Iteration 11, inertia 3105.184899216385\n",
      "Iteration 12, inertia 3103.563235326072\n",
      "Iteration 13, inertia 3101.6082306887192\n",
      "Iteration 14, inertia 3101.5337266391016\n",
      "Iteration 15, inertia 3101.5033675528903\n",
      "Iteration 16, inertia 3101.4950548154347\n",
      "Iteration 17, inertia 3101.48917216865\n",
      "Converged at iteration 17: strict convergence.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create TfidfVectorizer using the same options as with the CountVectorizer\n",
    "tfidf_vec = TfidfVectorizer(max_features = 1000, stop_words='english')\n",
    "X = tfidf_vec.fit_transform(dataset.data)\n",
    "\n",
    "km_tfidf = KMeans(n_clusters=4, init='k-means++', max_iter=100, n_init=1, verbose=True, random_state=2021)\n",
    "X_tfidf = X\n",
    "km_tfidf.fit(X_tfidf)\n",
    "labels_tfidf = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.480\n",
      "Completeness: 0.573\n",
      "V-measure: 0.522\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_tfidf, km_tfidf.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_tfidf, km_tfidf.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_tfidf, km_tfidf.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that whilst the KMeans algorithm produces better scores using the tfidf vectoriser relative to the count vectorizer, they are far from perfect and additional feature engineering and/or tuning would be required if being analysed in more than just a training scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonD",
   "language": "python",
   "name": "pythond"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
